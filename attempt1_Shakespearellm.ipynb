{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOAH4l0rSLDKD9t7rm6Nw9K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhaanupriyaranjit/DLProject/blob/main/Shakespearellm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYtKR2r_kJOR",
        "outputId": "2ea820bc-3d3c-427a-9c0d-3b9c4c8bb595"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision matplotlib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmqwFQ9MlVtU",
        "outputId": "0fb16891-3f20-4d62-c15a-ca6e582d2e48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "GPU available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset prep\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "data = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
        "print(f\"Dataset length: {len(data):,} characters\")\n",
        "print(data[:500])  #to check few lines\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQXkj9Lr09PD",
        "outputId": "48ffcbdc-2ad8-4c0a-8b95-c3e31c2f8f39"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset length: 1,115,394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization step\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "#configuration class for model hyperparameters-Batch Size B=128 and block size, N=128\n",
        "class Config:\n",
        "    def __init__(self, batch_size=128, block_size=128):\n",
        "        self.batch_size = batch_size\n",
        "        self.block_size = block_size\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Emits batches of characters.\n",
        "\n",
        "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, data):\n",
        "\n",
        "        self.data = data\n",
        "        self.block_size = config.block_size\n",
        "        self.batch_size = config.batch_size\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        chars = sorted(list(set(self.data))) # get characters from the input data\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }  # map integer indices to characters\n",
        "        self.vocab_size = len(chars)\n",
        "        print(f\"Vocab size: {self.vocab_size} unique characters\")\n",
        "\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "  #to know,from this text, how many training examples (input x,target pairs,y) can it make, ie. how many valid training smaples it can make\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        # encode every character to an integer\n",
        "        # return the chunk and the shifted version as tensors\n",
        "        chunk = self.data[idx : idx + self.block_size + 1]\n",
        "        encoded = torch.tensor([self.stoi[c] for c in chunk], dtype=torch.long)\n",
        "        x = encoded[:-1]\n",
        "        y = encoded[1:]\n",
        "        return x.to(self.device), y.to(self.device)"
      ],
      "metadata": {
        "id": "UJ93atwi1_Vp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to verify tokenization part\n",
        "\n",
        "#for training and validation-train set(90% of the data for training) and validation set(10% of data)\n",
        "n = int(0.9 * len(data))\n",
        "train_text = data[:n]\n",
        "val_text   = data[n:]\n",
        "\n",
        "cfg = Config(batch_size=128, block_size=128)\n",
        "train_dataset = CharDataset(cfg, train_text)\n",
        "val_dataset   = CharDataset(cfg, val_text)\n",
        "\n",
        "x, y = train_dataset[0]\n",
        "print(\"x shape:\", x.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n",
        "itos = train_dataset.itos\n",
        "print(\"Input  (x):\", ''.join([itos[i.item()] for i in x[:60]]))\n",
        "print(\"Target (y):\", ''.join([itos[i.item()] for i in y[:60]]))\n",
        "\n",
        "\n",
        "#to simulate a batch\n",
        "def get_batch(dataset, batch_size=cfg.batch_size):\n",
        "    ix = torch.randint(len(dataset), (batch_size,))\n",
        "    x = torch.stack([dataset[i][0] for i in ix])\n",
        "    y = torch.stack([dataset[i][1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch(train_dataset)\n",
        "print(\"Batch shapes:\", xb.shape, yb.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "jvt3ykzfOmpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87db69d1-bd3e-409c-c695-e3e6dc852a35"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 65 unique characters\n",
            "Vocab size: 61 unique characters\n",
            "x shape: torch.Size([128])\n",
            "y shape: torch.Size([128])\n",
            "Input  (x): First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "Target (y): irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "Batch shapes: torch.Size([128, 128]) torch.Size([128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#week 2\n",
        "#Model Architecture part begins here - model config setting, single attentionhead, multiattention head,feed forward-mlp,transformer block-full shakespeare gpt model\n",
        "\n",
        "import math\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "#to set the configuration of the model- vocab_size: how many unique characters, block_size: sequence length (N), n_layer: how many transformer blocks, n_head: how many attention heads,n_embd: embedding dimension,dropout: dropout rate\n",
        "class GPTConfig:\n",
        "\n",
        "# setting as per proj req - $12$ layers, $8$ attention heads, and $768$ embedding dimensions,\n",
        "    def __init__(self, vocab_size, block_size,\n",
        "                 n_layer=12, n_head=8, n_embd=768, dropout=0.1):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.dropout = dropout\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVm6GOM5rtif",
        "outputId": "1922f658-4d16-455b-a0aa-4a3c0c986335"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#single attention head(causal)- projects input x to Q, K, V, applies scaled dot product attention,and uses a causal mask so tokens cannot attend to the future(no cheating as it cannot see future tokens)\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg: GPTConfig, head_size: int):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(cfg.n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(cfg.n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(cfg.n_embd, head_size, bias=False)\n",
        "\n",
        "        #to precompute a causal mask of shape (block_size, block_size)\n",
        "        mask = torch.tril(torch.ones(cfg.block_size, cfg.block_size))\n",
        "        self.register_buffer(\"mask\", mask)  #not a parameter, but moves with device\n",
        "\n",
        "        self.dropout = nn.Dropout(cfg.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        #linear projections\n",
        "        k = self.key(x)   # (B, T, head_size)\n",
        "        q = self.query(x) # (B, T, head_size)\n",
        "        v = self.value(x) # (B, T, head_size)\n",
        "\n",
        "        #to compute attention scores: (B, T, T)\n",
        "        att = q @ k.transpose(-2, -1) / math.sqrt(k.shape[-1])\n",
        "\n",
        "        #to apply causal mask so position i cannot see positions > i(future positions)\n",
        "        att = att.masked_fill(self.mask[:T, :T] == 0, float(\"-inf\"))\n",
        "\n",
        "        #to apply softmax over last dimension-attention weights\n",
        "        att = F.softmax(att, dim=-1)\n",
        "\n",
        "        att = self.dropout(att)\n",
        "\n",
        "        #attention-weighted sum of values\n",
        "        out = att @ v  #(B, T, head_size)\n",
        "        return out"
      ],
      "metadata": {
        "id": "xKVrZv5TGYTG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#multihead attention part- it runs several attention heads(8 here) in parallel, concatenate their outputs and project back to embedding dimension\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "#embedding dimension = number_of_heads Ã— head_size, so head_size = n_embd // n_head as each head needs to project Q, K, V into equal-sized subspaces\n",
        "\n",
        "    def __init__(self, cfg: GPTConfig):\n",
        "        super().__init__()\n",
        "        assert cfg.n_embd % cfg.n_head == 0   #to prevent invalid configurations\n",
        "        head_size = cfg.n_embd // cfg.n_head\n",
        "        self.heads = nn.ModuleList([Head(cfg, head_size) for _ in range(cfg.n_head)])\n",
        "        self.proj = nn.Linear(cfg.n_embd, cfg.n_embd)\n",
        "        self.dropout = nn.Dropout(cfg.dropout)\n",
        "\n",
        "#to concatenate outputs from each head on the channel dimension\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)  #(B, T, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Ti4VbdSRJoB5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feed forward network-mlp- simple 2 layer mlp applied to each position separately-- Linear layer-GELU-Linear-Dropout\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(cfg.n_embd, 4 * cfg.n_embd), #input embedding- C dim , in linear layer -expands features so 4*C(768--> 3072)\n",
        "            #Gaussian Error Linear Unit-standard activation function fro trasnformers to add non linearity\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * cfg.n_embd, cfg.n_embd), #2nd linear layer - back to original embdeeing size\n",
        "            nn.Dropout(cfg.dropout), #to add regularization\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "j0XWO81xKyCM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one transformer block- x = x + self.CausalSelfAttn(self.LayerNorm_1(x)),out = x + self.MLP(self.LayerNorm_2(x))\n",
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(cfg.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(cfg.n_embd)\n",
        "        self.attn = MultiHeadAttention(cfg)\n",
        "        self.mlp = FeedForward(cfg)\n",
        "\n",
        "#self attention with residual connection and mlp with residual connectio\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "DYN__-eFMZbj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#implemnetation of the full shakespeare style GPT like model-decoder only transformer for character-level language modeling\n",
        "class ShakespeareGPT(nn.Module):\n",
        "    def __init__(self, cfg: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        #token and position embeddings\n",
        "        self.WTE = nn.Embedding(cfg.vocab_size, cfg.n_embd)#converts each  int id of char to vector emb of dim 768\n",
        "        self.WPE = nn.Embedding(cfg.block_size, cfg.n_embd)#block szie, T=N=128, WPE-->0,1,2, ...,block_size - 1   (0 to 127 for block_size=128)\n",
        "\n",
        "        #randomly 0s some dimensions during training for regularization\n",
        "        self.dropout = nn.Dropout(cfg.dropout)\n",
        "\n",
        "        #12 layers of transformer blocks\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layer)])\n",
        "\n",
        "        #final layer norm and LM head\n",
        "        self.Final_LayerNorm = nn.LayerNorm(cfg.n_embd)\n",
        "        self.LM_Head = nn.Linear(cfg.n_embd, cfg.vocab_size, bias=False)\n",
        "\n",
        "#B=128 batch size, T=N=128-block size(each seq length), C=768-channel-emb dim\n",
        "#idx: (B, T)(128,128) integer token ids,targets: (B, T) integer next-token ids (for loss)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        #to prevent error and make sure input is nt longer than the model can handle\n",
        "        assert T <= self.cfg.block_size, \"Sequence length > block_size\"\n",
        "\n",
        "        #token embeddings\n",
        "        tok_emb = self.WTE(idx)  #(B, T, C)\n",
        "\n",
        "        #position embeddings\n",
        "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)  #(1,T)\n",
        "        pos_emb = self.WPE(pos)                                   #(1,T,C)\n",
        "\n",
        "        #add token and pos emb + dropout\n",
        "        x = self.dropout(tok_emb + pos_emb)                       #(B,T,C)\n",
        "\n",
        "        #to pass through 12 transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)                                          #(B-128,T-128,C-768)\n",
        "\n",
        "        #final layer norm\n",
        "        x = self.Final_LayerNorm(x)\n",
        "\n",
        "        #language modeling head-gives prob for each char\n",
        "        logits = self.LM_Head(x)                                  #(B,T,vocab_size-65)\n",
        "\n",
        "        #optional loss calculation for training-next char pred loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            #flatten B,T into 1 dimension\n",
        "            logits_flat = logits.view(B * T, self.cfg.vocab_size) #(128*128,65)\n",
        "            targets_flat = targets.view(B * T)                    #(128*128)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "  #to implement this auto-regressive behavior-for the repeat loop- a method to generate tokens given a prompt\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "      self.eval()\n",
        "\n",
        "      for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -self.cfg.block_size:] #crop sequence to last block_size\n",
        "\n",
        "        #forward pass\n",
        "        logits, _ = self(idx_cond)\n",
        "\n",
        "        #take logits of last time step\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        #turn into probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        #sample next character\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        #append\n",
        "        idx = torch.cat((idx, next_token), dim=1)\n",
        "\n",
        "      return idx\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "26getTmnOjGK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the prev gpt like model code, check if custom manual weight initialization is needed ot not-currently, just going with pytorch default weight initialization"
      ],
      "metadata": {
        "id": "ZwgcwkSAWb74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for verification part-model forward pass\n",
        "\n",
        "vocab_size = train_dataset.get_vocab_size()\n",
        "cfg_model = GPTConfig(\n",
        "    vocab_size=vocab_size,\n",
        "    block_size=128,\n",
        "    n_layer=12,\n",
        "    n_head=8,\n",
        "    n_embd=768,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "model = ShakespeareGPT(cfg_model).to(device)\n",
        "\n",
        "#to take  abatch\n",
        "xb, yb = get_batch(train_dataset)\n",
        "\n",
        "#to run a forward pass\n",
        "with torch.no_grad():\n",
        "    logits, loss = model(xb.to(device), yb.to(device))\n",
        "\n",
        "print(\"Logits shape:\", logits.shape)\n",
        "print(\"Loss:\", loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZiCda6nZq7k",
        "outputId": "a1a6cbf1-5bcb-4861-b8ac-fd596be970a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([128, 128, 65])\n",
            "Loss: 4.293853759765625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"O God\"\n",
        "encoded = torch.tensor([[train_dataset.stoi[c] for c in prompt]], device=device)\n",
        "\n",
        "generated = model.generate(encoded, max_new_tokens=20)\n",
        "\n",
        "decoded = ''.join([train_dataset.itos[int(i)] for i in generated[0]])\n",
        "\n",
        "print(decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRewerlvsQtM",
        "outputId": "aa01076f-e8e4-476c-8f7b-e69987674f35"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O GodGw&Hh&SQ$kXmlI'H!fA$\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Trainig Part"
      ],
      "metadata": {
        "id": "Cc-KSKGOsfOc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
